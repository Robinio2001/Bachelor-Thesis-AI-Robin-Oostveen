{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V100","authorship_tag":"ABX9TyO9Qcy5RPRiOwF2HwOY9RCC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IxEpetjAVVDJ","executionInfo":{"status":"ok","timestamp":1686737604986,"user_tz":-120,"elapsed":24145,"user":{"displayName":"Robin Oostveen","userId":"05439229719617169461"}},"outputId":"95f46da4-6543-4ca1-811c-98b00c7022bd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["import pandas as pd\n","import nltk\n","import sklearn\n","import numpy as np\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install tensorflow_text"],"metadata":{"id":"k_kRQhznV2r2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","import tensorflow_hub as hub\n","import tensorflow_text as text"],"metadata":{"id":"tZxzE2f4V0Gp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import sys\n","sys.path.insert(0, '/content/drive/My Drive/Bachelor Scriptie KI/Programming/Notebooks')"],"metadata":{"id":"6vbro8tZDWbF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import multi_class_performance_eval as mce"],"metadata":{"id":"i2torAZDDXmk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Setting the random seeds for reproducability\n","import random\n","np.random.seed(42)\n","tf.random.set_seed(42)\n","random.seed(42)"],"metadata":{"id":"AayjC6MipqhI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train = pd.read_csv(\"/content/drive/My Drive/Bachelor Scriptie KI/FinancialPhraseBank-v1.0/train.csv\", sep=\",\", names=[\"Sentence\", \"Sentiment\"], encoding=\"utf-8\", skiprows=[0])\n","test = pd.read_csv(\"/content/drive/My Drive/Bachelor Scriptie KI/FinancialPhraseBank-v1.0/test.csv\", sep=\",\", names=[\"Sentence\", \"Sentiment\"], encoding=\"utf-8\", skiprows=[0])\n","val = pd.read_csv(\"/content/drive/My Drive/Bachelor Scriptie KI/FinancialPhraseBank-v1.0/validation.csv\", sep=\",\", names=[\"Sentence\", \"Sentiment\"], encoding=\"utf-8\", skiprows=[0])"],"metadata":{"id":"Ss5GziCw3ZHI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def label_distribution(dataframe):\n","  \"\"\"\n","  Determine the distribution of labels in the dataframe.\n","\n","  param dataframe: Pandas DataFrame contains the sentences and sentiment labels.\n","  \"\"\"\n","  total = 0\n","  pos = 0\n","  neu = 0\n","  neg = 0\n","\n","  for label in dataframe.Sentiment:\n","    if label == 0:\n","      neg += 1\n","    elif label == 1:\n","      neu += 1\n","    else:\n","      pos += 1\n","    total += 1\n","  return pos, neu, neg, total"],"metadata":{"id":"6GejxYHY08ZD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pos, neu, neg, total = label_distribution(train)\n","print(\"Positive: \", str(round(pos/total*100, 2)))\n","print(\"Neutral: \", str(round(neu/total*100, 2)))\n","print(\"Negative: \", str(round(neg/total*100, 2)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iOa3dEN013qB","executionInfo":{"status":"ok","timestamp":1686737616913,"user_tz":-120,"elapsed":39,"user":{"displayName":"Robin Oostveen","userId":"05439229719617169461"}},"outputId":"36555292-1f5f-4eea-fd10-5adaf80c12fa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Positive:  28.2\n","Neutral:  59.6\n","Negative:  12.2\n"]}]},{"cell_type":"code","source":["pos, neu, neg, total = label_distribution(test)\n","print(\"Positive: \", str(round(pos/total*100, 2)))\n","print(\"Neutral: \", str(round(neu/total*100, 2)))\n","print(\"Negative: \", str(round(neg/total*100, 2)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kYT_Ighb16-Z","executionInfo":{"status":"ok","timestamp":1686737616915,"user_tz":-120,"elapsed":39,"user":{"displayName":"Robin Oostveen","userId":"05439229719617169461"}},"outputId":"15ae8ae5-e1da-4889-a2b0-2b5d6366e8e1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Positive:  26.8\n","Neutral:  58.76\n","Negative:  14.43\n"]}]},{"cell_type":"code","source":["pos, neu, neg, total = label_distribution(val)\n","print(\"Positive: \", str(round(pos/total*100, 2)))\n","print(\"Neutral: \", str(round(neu/total*100, 2)))\n","print(\"Negative: \", str(round(neg/total*100, 2)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HM2cvho7197B","executionInfo":{"status":"ok","timestamp":1686737616915,"user_tz":-120,"elapsed":37,"user":{"displayName":"Robin Oostveen","userId":"05439229719617169461"}},"outputId":"f0f3144c-e1e7-48cc-883a-86aecca15d32"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Positive:  28.87\n","Neutral:  58.56\n","Negative:  12.58\n"]}]},{"cell_type":"code","source":["X_train = train.Sentence\n","X_test = test.Sentence\n","X_val = val.Sentence"],"metadata":{"id":"7dLg0IVTZ7UJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#The categorical cross-entropy loss expects the sentiments to be one-hot encoded.\n","y_train = tf.keras.utils.to_categorical(train.Sentiment.tolist())\n","y_test = tf.keras.utils.to_categorical(test.Sentiment.tolist())\n","y_val = tf.keras.utils.to_categorical(val.Sentiment.tolist())"],"metadata":{"id":"oXc79Zr5Z-6F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Load the BERT preprocesser and encoder into KerasLayers.\n","bert_preprocess = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n","bert_encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\")"],"metadata":{"id":"sqMZgjQzbLkB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_BERT_CNN(preprocesser, encoder, kernel_size, filters, activation, padding):\n","  \"\"\"\n","  Creates a Tensorflow model that incorporates BERT preprocessing and encoding layers.\n","  Adds a CNN behind it and a Dense layer with 3 neurons for classification.\n","\n","  param preprocesser: (KerasLayer) a layer containing the BERT preprocesser.\n","  param encoder: (KerasLayer) a layer containing the BERT encoder.\n","  param kernel_size: (int) the size of the kernel used in the Convolutional layer.\n","  param filters: (int) the amount of filters used in the Convolutional layer.\n","  param activation: (str) activation function to be used in the Convolutional layer.\n","  param padding: (str) padding used in Convolutional layer.\n","  \"\"\"\n","  #Define the BERT layers\n","  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text_input')\n","  preprocessing_bert = preprocesser(text_input)\n","  encoder_bert = encoder(preprocessing_bert)\n","  outputs = encoder_bert['sequence_output']\n","\n","  #Define the CNN that uses the BERT embeddings\n","  conv = tf.keras.layers.Conv1D(kernel_size=kernel_size, filters=filters, padding=padding, activation=activation, name=\"conv1d\")(outputs)\n","  conv = tf.keras.layers.GlobalMaxPool1D(name=\"pool\")(conv)\n","\n","  #Define Dense output layer\n","  ff = tf.keras.layers.Dense(3, activation='softmax', name=\"output\")(conv)\n","\n","  classifier = tf.keras.Model(inputs=[text_input], outputs=[ff])\n","  return classifier"],"metadata":{"id":"JzkulLKACkPO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Fine-tuning of hyperparameters"],"metadata":{"id":"0U9vONkdSIGT"}},{"cell_type":"code","source":["kernel_sizes = [1, 2, 4, 8, 10]\n","filters = [64, 128, 256, 512]\n","\n","for size in kernel_sizes:\n","  for filter in filters:\n","    #Create the model and compile it.\n","    model = create_BERT_CNN(bert_preprocess, bert_encoder, kernel_size=size, filters=filter, activation=\"relu\", padding=\"same\")\n","    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","    #We make use of early stopping and save the best weights.\n","    early_stop =  tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode=\"min\", patience=5)\n","    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"./BERT_conv_model\", save_best_only=True)\n","\n","    print(\"Training model with kernel size \", str(size), \" and filters \", str(filter), \"\\n \\n \\n\")\n","\n","    hist = model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val), callbacks=[early_stop, checkpoint_cb])\n","\n","    train_acc_hist = hist.history['accuracy']\n","    train_acc = 0\n","    count = 0\n","    for i in train_acc_hist:\n","      train_acc += i\n","      count += 1\n","    train_acc = train_acc / count\n","    print(\"Training accuracy was: \", str(train_acc))\n","\n","    val_acc_hist = hist.history['val_accuracy']\n","    val_acc = 0\n","    count = 0\n","    for i in val_acc_hist:\n","      val_acc += i\n","      count += 1\n","    val_acc = val_acc / count\n","    print(\"Validation accuracy was: \", str(val_acc))\n","\n","    #Roll back to model found performing best on validation set during training.\n","    model.load_weights(\"./BERT_conv_model\")\n","\n","    #Generate predictions using this model\n","    predictions = model.predict(X_test)\n","\n","    #Convert the one-hot encoded predictions into a number.\n","    y_pred = []\n","    for pred in predictions:\n","      label = np.argmax(pred)\n","      y_pred.append(label)\n","\n","    #Convert the one-hot encoded real sentiment labels into numbers.\n","    y_real = []\n","    for encoding in y_test:\n","      label = np.argmax(encoding)\n","      y_real.append(label)\n","\n","    #Evaluate performance of the model using the current set of hyperparameters.\n","    evaluation = mce.evaluate_performance(y_pred, y_real)\n","\n","    print(\"Tested model with kernel size \", str(size), \" and filters \", str(filter), \" on the test set\")\n","    for metric in evaluation:\n","      print(metric, \": \", evaluation[metric], \"\\n\")\n","    print(\"\\n \\n \\n\")\n","\n","\n","\n","\n"],"metadata":{"id":"d75VBMPBmRnh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Further fine-tuning of the hyperparameters\n","kernel_sizes = [1, 2, 3, 4]\n","filters = [200, 220, 256, 280, 300]\n","\n","for size in kernel_sizes:\n","  for filter in filters:\n","        model = create_BERT_CNN(bert_preprocess, bert_encoder, kernel_size=size, filters=filter, activation=\"relu\", padding=\"same\")\n","        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","        early_stop =  tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode=\"min\", patience=5)\n","        checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"./BERT_conv_model\", save_best_only=True)\n","\n","        print(\"Training model with kernel size \", str(size), \" and filters \", str(filter), \"\\n \\n \\n\")\n","\n","        hist = model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val), callbacks=[early_stop, checkpoint_cb])\n","\n","        train_acc_hist = hist.history['accuracy']\n","        train_acc = 0\n","        count = 0\n","        for i in train_acc_hist:\n","          train_acc += i\n","          count += 1\n","        train_acc = train_acc / count\n","        print(\"Training accuracy was: \", str(train_acc))\n","\n","        val_acc_hist = hist.history['val_accuracy']\n","        val_acc = 0\n","        count = 0\n","        for i in val_acc_hist:\n","          val_acc += i\n","          count += 1\n","        val_acc = val_acc / count\n","        print(\"Validation accuracy was: \", str(val_acc))\n","\n","        #Roll back to model found performing best on validation set during training.\n","        model.load_weights(\"./BERT_conv_model\")\n","\n","        #Generate predictions using this model\n","        predictions = model.predict(X_test)\n","\n","        y_pred = []\n","        for pred in predictions:\n","          label = np.argmax(pred)\n","          y_pred.append(label)\n","\n","        y_real = []\n","        for encoding in y_test:\n","          label = np.argmax(encoding)\n","          y_real.append(label)\n","\n","        #Evaluate performance of the model using the current set of hyperparameters\n","        evaluation = mce.evaluate_performance(y_pred, y_real)\n","\n","        print(\"Tested model with kernel size \", str(size), \" and filters \", str(filter), \" on the test set\")\n","        for metric in evaluation:\n","          print(metric, \": \", evaluation[metric], \"\\n\")\n","        print(\"\\n \\n \\n\")"],"metadata":{"id":"og_ztgMc6a9w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Tune learning rate, the standard is 0.001 according to documentation for Adam optimizer.\n","learning_rates = [0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3]\n","for lr in learning_rates:\n","    model = create_BERT_CNN(bert_preprocess, bert_encoder, kernel_size=1, filters=256, activation=\"relu\", padding=\"same\")\n","    optim = tf.keras.optimizers.Adam(learning_rate=lr)\n","    model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'])\n","\n","    early_stop =  tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode=\"min\", patience=5)\n","    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"./BERT_conv_model\", save_best_only=True)\n","\n","    print(\"Training model with learning rate: \", str(lr), \"\\n \\n \\n\")\n","\n","    hist = model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val), callbacks=[early_stop, checkpoint_cb])\n","\n","    train_acc_hist = hist.history['accuracy']\n","    train_acc = 0\n","    count = 0\n","    for i in train_acc_hist:\n","      train_acc += i\n","      count += 1\n","    train_acc = train_acc / count\n","    print(\"Training accuracy was: \", str(train_acc))\n","\n","    val_acc_hist = hist.history['val_accuracy']\n","    val_acc = 0\n","    count = 0\n","    for i in val_acc_hist:\n","      val_acc += i\n","      count += 1\n","    val_acc = val_acc / count\n","    print(\"Validation accuracy was: \", str(val_acc))\n","\n","    #Roll back to model found performing best on validation set during training.\n","    model.load_weights(\"./BERT_conv_model\")\n","\n","    #Generate predictions using this model.\n","    predictions = model.predict(X_test)\n","\n","    y_pred = []\n","    for pred in predictions:\n","      label = np.argmax(pred)\n","      y_pred.append(label)\n","\n","    y_real = []\n","    for encoding in y_test:\n","      label = np.argmax(encoding)\n","      y_real.append(label)\n","\n","    #Evaluate performance of the model using the current set of hyperparameters.\n","    evaluation = mce.evaluate_performance(y_pred, y_real)\n","\n","    print(\"Tested model with learning rate  \", str(lr), \" on the test set\")\n","    for metric in evaluation:\n","      print(metric, \": \", evaluation[metric], \"\\n\")\n","    print(\"\\n \\n \\n\")"],"metadata":{"id":"RkTxChps7iUI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Check the effect of dropout.\n","dropout_chances = [0.05, 0.1, 0.3, 0.5]\n","for chance in dropout_chances:\n","    model = create_BERT_CNN(bert_preprocess, bert_encoder, kernel_size=1, filters=220, activation=\"relu\", padding=\"same\", dropout=chance)\n","    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","    early_stop =  tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode=\"max\", patience=5)\n","\n","    print(\"\\n \\n\", \"Training model with dropout \", str(chance), \"\\n \\n \\n\")\n","\n","    hist = model.fit(X_train, y_train, batch_size=16, epochs=50, validation_data=(X_val, y_val), callbacks=[early_stop])\n","\n","    train_acc_hist = hist.history['accuracy']\n","    train_acc = 0\n","    count = 0\n","    for i in train_acc_hist:\n","      train_acc += i\n","      count += 1\n","    train_acc = train_acc / count\n","    print(\"Training accuracy was: \", str(train_acc))\n","\n","    val_acc_hist = hist.history['val_accuracy']\n","    val_acc = 0\n","    count = 0\n","    for i in val_acc_hist:\n","      val_acc += i\n","      count += 1\n","    val_acc = val_acc / count\n","    print(\"Validation accuracy was: \", str(val_acc))"],"metadata":{"id":"xgm_5jIv8Z3Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Check the influence of different activation functions.\n","activation_functions = [\"relu\", \"elu\"]\n","\n","for activation in activation_functions:\n","    model = create_BERT_CNN(bert_preprocess, bert_encoder, kernel_size=1, filters=220, activation=activation, padding=\"same\", dropout=chance)\n","    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","    early_stop =  tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode=\"max\", patience=5)\n","\n","    print(\"\\n \\n\", \"Training model with activation function \", str(activation), \"\\n \\n \\n\")\n","\n","    hist = model.fit(X_train, y_train, batch_size=16, epochs=50, validation_data=(X_val, y_val), callbacks=[early_stop])\n","\n","    train_acc_hist = hist.history['accuracy']\n","    train_acc = 0\n","    count = 0\n","    for i in train_acc_hist:\n","      train_acc += i\n","      count += 1\n","    train_acc = train_acc / count\n","    print(\"Training accuracy was: \", str(train_acc))\n","\n","    val_acc_hist = hist.history['val_accuracy']\n","    val_acc = 0\n","    count = 0\n","    for i in val_acc_hist:\n","      val_acc += i\n","      count += 1\n","    val_acc = val_acc / count\n","    print(\"Validation accuracy was: \", str(val_acc))"],"metadata":{"id":"BpQlfwEzCOgD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Check the effect of different padding settings.\n","paddings = [\"valid\", \"same\", \"causal\"]\n","for padding in paddings:\n","    model = create_BERT_CNN(bert_preprocess, bert_encoder, kernel_size=1, filters=220, activation=\"relu\", padding=padding, dropout=chance)\n","    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","    early_stop =  tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode=\"max\", patience=5)\n","\n","    print(\"\\n \\n\", \"Training model with padding \", str(padding), \"\\n \\n \\n\")\n","\n","    hist = model.fit(X_train, y_train, batch_size=16, epochs=50, validation_data=(X_val, y_val), callbacks=[early_stop])\n","\n","    train_acc_hist = hist.history['accuracy']\n","    train_acc = 0\n","    count = 0\n","    for i in train_acc_hist:\n","      train_acc += i\n","      count += 1\n","    train_acc = train_acc / count\n","    print(\"Training accuracy was: \", str(train_acc))\n","\n","    val_acc_hist = hist.history['val_accuracy']\n","    val_acc = 0\n","    count = 0\n","    for i in val_acc_hist:\n","      val_acc += i\n","      count += 1\n","    val_acc = val_acc / count\n","    print(\"Validation accuracy was: \", str(val_acc))"],"metadata":{"id":"v5ynkHBaK4v5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_BERT_CNN_doubleblock(preprocesser, encoder, kernel_size, filters, activation, padding):\n","  \"\"\"\n","  Creates a Tensorflow model with BERT embeddings, but this time with 2 blocks of Conv1D + GlobalMaxPool1D layers.\n","  param preprocesser: (KerasLayer) a layer containing the BERT preprocesser.\n","  param encoder: (KerasLayer) a layer containing the BERT encoder.\n","  param kernel_size: (int) the size of the kernel used in the Convolutional layer.\n","  param filters: (int) the amount of filters used in the Convolutional layer.\n","  param activation: (str) activation function to be used in the Convolutional layer.\n","  param padding: (str) padding used in Convolutional layer.\n","  \"\"\"\n","  #Define the BERT layers\n","  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text_input')\n","  preprocessing_bert = preprocesser(text_input)\n","  encoder_bert = encoder(preprocessing_bert)\n","  outputs = encoder_bert['sequence_output']\n","\n","  #Define the CNN that uses the BERT embeddings\n","  conv = tf.keras.layers.Conv1D(kernel_size=kernel_size, filters=filters, padding=padding, activation=activation, name=\"conv1d.1\")(outputs)\n","  conv = tf.keras.layers.MaxPool1D(name=\"pool1\")(conv)\n","  conv = tf.keras.layers.Conv1D(kernel_size=kernel_size, filters=(filters/2), padding=padding, activation=activation, name=\"conv1d.2\")(conv)\n","  conv= tf.keras.layers.GlobalMaxPool1D(name=\"pool2\")(conv)\n","\n","  #Define Dense output layer\n","  ff = tf.keras.layers.Dense(3, activation='softmax', name=\"output\")(conv)\n","\n","  classifier = tf.keras.Model(inputs=[text_input], outputs=[ff])\n","  return classifier"],"metadata":{"id":"htI2iMcKPtJ4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["double_conv_classifier = create_BERT_CNN_doubleblock(bert_preprocess, bert_encoder, 1, 256, \"relu\", \"same\")"],"metadata":{"id":"ZUfRvoUQllbQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["double_conv_classifier.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GT0gfM7MlzJ3","executionInfo":{"status":"ok","timestamp":1686246523165,"user_tz":-120,"elapsed":366,"user":{"displayName":"Robin Oostveen","userId":"05439229719617169461"}},"outputId":"6ea1ae86-200d-4434-b259-aca7283d3f85"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," text_input (InputLayer)        [(None,)]            0           []                               \n","                                                                                                  \n"," keras_layer (KerasLayer)       {'input_word_ids':   0           ['text_input[0][0]']             \n","                                (None, 128),                                                      \n","                                 'input_mask': (Non                                               \n","                                e, 128),                                                          \n","                                 'input_type_ids':                                                \n","                                (None, 128)}                                                      \n","                                                                                                  \n"," keras_layer_1 (KerasLayer)     {'default': (None,   109482241   ['keras_layer[0][0]',            \n","                                768),                             'keras_layer[0][1]',            \n","                                 'encoder_outputs':               'keras_layer[0][2]']            \n","                                 [(None, 128, 768),                                               \n","                                 (None, 128, 768),                                                \n","                                 (None, 128, 768),                                                \n","                                 (None, 128, 768),                                                \n","                                 (None, 128, 768),                                                \n","                                 (None, 128, 768),                                                \n","                                 (None, 128, 768),                                                \n","                                 (None, 128, 768),                                                \n","                                 (None, 128, 768),                                                \n","                                 (None, 128, 768),                                                \n","                                 (None, 128, 768),                                                \n","                                 (None, 128, 768)],                                               \n","                                 'pooled_output': (                                               \n","                                None, 768),                                                       \n","                                 'sequence_output':                                               \n","                                 (None, 128, 768)}                                                \n","                                                                                                  \n"," conv1d.1 (Conv1D)              (None, 128, 256)     196864      ['keras_layer_1[0][14]']         \n","                                                                                                  \n"," pool1 (MaxPooling1D)           (None, 64, 256)      0           ['conv1d.1[0][0]']               \n","                                                                                                  \n"," conv1d.2 (Conv1D)              (None, 64, 128)      32896       ['pool1[0][0]']                  \n","                                                                                                  \n"," pool2 (GlobalMaxPooling1D)     (None, 128)          0           ['conv1d.2[0][0]']               \n","                                                                                                  \n"," output (Dense)                 (None, 3)            387         ['pool2[0][0]']                  \n","                                                                                                  \n","==================================================================================================\n","Total params: 109,712,388\n","Trainable params: 230,147\n","Non-trainable params: 109,482,241\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","source":["optim = tf.keras.optimizers.Adam(learning_rate=0.0003)\n","\n","double_conv_classifier.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'])\n","\n","early_stop =  tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode=\"min\", patience=5)\n","checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"./BERT_conv_model_doubleblock\", save_best_only=True)"],"metadata":{"id":"OBT6QwCtxGlY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Training model with 2 convolutional layers \\n \\n\")\n","hist = double_conv_classifier.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val), callbacks=[early_stop, checkpoint_cb])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bDjkEobcHgRK","executionInfo":{"status":"ok","timestamp":1686246910191,"user_tz":-120,"elapsed":373680,"user":{"displayName":"Robin Oostveen","userId":"05439229719617169461"}},"outputId":"c44c1b8e-e360-4c70-afe3-2fa8bf2b7a59"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training model with 2 convolutional layers \n"," \n","\n","Epoch 1/50\n","122/122 [==============================] - ETA: 0s - loss: 0.7388 - accuracy: 0.6726"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 368). These functions will not be directly callable after loading.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r122/122 [==============================] - 65s 330ms/step - loss: 0.7388 - accuracy: 0.6726 - val_loss: 0.6504 - val_accuracy: 0.7237\n","Epoch 2/50\n","121/122 [============================>.] - ETA: 0s - loss: 0.5336 - accuracy: 0.7748"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 368). These functions will not be directly callable after loading.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r122/122 [==============================] - 39s 317ms/step - loss: 0.5332 - accuracy: 0.7750 - val_loss: 0.5793 - val_accuracy: 0.7464\n","Epoch 3/50\n","121/122 [============================>.] - ETA: 0s - loss: 0.3999 - accuracy: 0.8435"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 368). These functions will not be directly callable after loading.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r122/122 [==============================] - 37s 307ms/step - loss: 0.3996 - accuracy: 0.8437 - val_loss: 0.5536 - val_accuracy: 0.7608\n","Epoch 4/50\n","121/122 [============================>.] - ETA: 0s - loss: 0.3315 - accuracy: 0.8802"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 368). These functions will not be directly callable after loading.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r122/122 [==============================] - 37s 308ms/step - loss: 0.3318 - accuracy: 0.8798 - val_loss: 0.5417 - val_accuracy: 0.7567\n","Epoch 5/50\n","122/122 [==============================] - 22s 181ms/step - loss: 0.2708 - accuracy: 0.9167 - val_loss: 0.5631 - val_accuracy: 0.7588\n","Epoch 6/50\n","122/122 [==============================] - 23s 187ms/step - loss: 0.2179 - accuracy: 0.9422 - val_loss: 0.5489 - val_accuracy: 0.7588\n","Epoch 7/50\n","121/122 [============================>.] - ETA: 0s - loss: 0.1782 - accuracy: 0.9595"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 368). These functions will not be directly callable after loading.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r122/122 [==============================] - 39s 319ms/step - loss: 0.1780 - accuracy: 0.9595 - val_loss: 0.5276 - val_accuracy: 0.7608\n","Epoch 8/50\n","122/122 [==============================] - 22s 181ms/step - loss: 0.1392 - accuracy: 0.9778 - val_loss: 0.5395 - val_accuracy: 0.7649\n","Epoch 9/50\n","122/122 [==============================] - 23s 185ms/step - loss: 0.1103 - accuracy: 0.9897 - val_loss: 0.5425 - val_accuracy: 0.7608\n","Epoch 10/50\n","122/122 [==============================] - 22s 182ms/step - loss: 0.0876 - accuracy: 0.9933 - val_loss: 0.5937 - val_accuracy: 0.7526\n","Epoch 11/50\n","122/122 [==============================] - 22s 179ms/step - loss: 0.0718 - accuracy: 0.9956 - val_loss: 0.5415 - val_accuracy: 0.7649\n","Epoch 12/50\n","122/122 [==============================] - 22s 184ms/step - loss: 0.0568 - accuracy: 0.9974 - val_loss: 0.5739 - val_accuracy: 0.7649\n"]}]},{"cell_type":"code","source":["train_acc_hist = hist.history['accuracy']\n","train_acc = 0\n","count = 0\n","for i in train_acc_hist:\n","  train_acc += i\n","  count += 1\n","train_acc = train_acc / count\n","print(\"Training accuracy was: \", str(train_acc))\n","\n","val_acc_hist = hist.history['val_accuracy']\n","val_acc = 0\n","count = 0\n","for i in val_acc_hist:\n","  val_acc += i\n","  count += 1\n","val_acc = val_acc / count\n","print(\"Validation accuracy was: \", str(val_acc))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1vMVFR14JvDO","executionInfo":{"status":"ok","timestamp":1686247010070,"user_tz":-120,"elapsed":335,"user":{"displayName":"Robin Oostveen","userId":"05439229719617169461"}},"outputId":"bb9a6fb8-3190-406a-ef2a-485f2fae3b5a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training accuracy was:  0.9119367102781931\n","Validation accuracy was:  0.7561855614185333\n"]}]},{"cell_type":"code","source":["#Roll back to model found performing best on validation set during training.\n","double_conv_classifier.load_weights(\"./BERT_conv_model_doubleblock\")\n","\n","#Generate predictions using this model\n","predictions = double_conv_classifier.predict(X_test)\n","\n","y_pred = []\n","for pred in predictions:\n","  label = np.argmax(pred)\n","  y_pred.append(label)\n","\n","y_real = []\n","for encoding in y_test:\n","  label = np.argmax(encoding)\n","  y_real.append(label)\n","\n","#Evaluate performance of the model using the current set of hyperparameters\n","evaluation = mce.evaluate_performance(y_pred, y_real)\n","\n","print(\"Tested model with 2 convolutional layers on the test set\")\n","for metric in evaluation:\n","  print(metric, \": \", evaluation[metric], \"\\n\")\n","print(\"\\n \\n \\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hFCocwxKJ2rm","executionInfo":{"status":"ok","timestamp":1686247131670,"user_tz":-120,"elapsed":4608,"user":{"displayName":"Robin Oostveen","userId":"05439229719617169461"}},"outputId":"40d568f0-9115-4ad7-a577-376b530e9f7e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["16/16 [==============================] - 3s 153ms/step\n","Tested model with 2 convolutional layers on the test set\n","Accuracy :  80.0 \n","\n","Base Positive :  {'TP': 90, 'FP': 34, 'TN': 321, 'FN': 40} \n","\n","Base Neutral :  {'TP': 255, 'FP': 51, 'TN': 149, 'FN': 30} \n","\n","Base Negative :  {'TP': 43, 'FP': 12, 'TN': 403, 'FN': 27} \n","\n","Advanced Positive :  {'Precision': 0.7258064516129032, 'Recall': 0.6923076923076923, 'Specificity': 0.9042253521126761} \n","\n","Advanced Neutral :  {'Precision': 0.8333333333333334, 'Recall': 0.8947368421052632, 'Specificity': 0.745} \n","\n","Advanced Negative :  {'Precision': 0.7818181818181819, 'Recall': 0.6142857142857143, 'Specificity': 0.9710843373493976} \n","\n","Balanced Accuracy :  0.7337767495662232 \n","\n","F_Score :  0.4330611035115569 \n","\n","\n"," \n"," \n","\n"]}]},{"cell_type":"markdown","source":["### Adding an additional convolutional layer does not significantly increase the performance. To not unessecarily increase the model's complexity, we choose to keep one single convolutional layer."],"metadata":{"id":"gh8gR9PXKlwO"}},{"cell_type":"markdown","source":["# Test the final hyperparameter set on the test set to record results"],"metadata":{"id":"E8-UTM7_K2mW"}},{"cell_type":"code","source":["final_model = create_BERT_CNN(bert_preprocess, bert_encoder, kernel_size=1, filters=256, activation=\"relu\", padding=\"same\")\n","\n","optim_adam = tf.keras.optimizers.Adam(learning_rate=0.0003)\n","final_model.compile(loss='categorical_crossentropy', optimizer=optim_adam, metrics=['accuracy'])\n","\n","early_stop =  tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode=\"min\", patience=5)\n","checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"./final_BERT_conv_model\", save_best_only=True)\n","\n","print(\"Training the final BERT+CNN model... \\n \\n\")\n","\n","hist = final_model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val), callbacks=[early_stop, checkpoint_cb])\n","\n","train_acc_hist = hist.history['accuracy']\n","train_acc = 0\n","count = 0\n","for i in train_acc_hist:\n","  train_acc += i\n","  count += 1\n","train_acc = train_acc / count\n","print(\"Training accuracy was: \", str(train_acc))\n","\n","val_acc_hist = hist.history['val_accuracy']\n","val_acc = 0\n","count = 0\n","for i in val_acc_hist:\n","  val_acc += i\n","  count += 1\n","val_acc = val_acc / count\n","print(\"Validation accuracy was: \", str(val_acc))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1R2Nnf6rKHeG","executionInfo":{"status":"ok","timestamp":1686571212046,"user_tz":-120,"elapsed":381352,"user":{"displayName":"Robin Oostveen","userId":"05439229719617169461"}},"outputId":"e936003a-4589-4609-dd9e-f232770a26ee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training the final BERT+CNN model... \n"," \n","\n","Epoch 1/50\n","122/122 [==============================] - ETA: 0s - loss: 0.7531 - accuracy: 0.6664"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 367). These functions will not be directly callable after loading.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r122/122 [==============================] - 47s 343ms/step - loss: 0.7531 - accuracy: 0.6664 - val_loss: 0.6659 - val_accuracy: 0.6948\n","Epoch 2/50\n","121/122 [============================>.] - ETA: 0s - loss: 0.5531 - accuracy: 0.7627"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 367). These functions will not be directly callable after loading.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r122/122 [==============================] - 55s 450ms/step - loss: 0.5527 - accuracy: 0.7629 - val_loss: 0.6005 - val_accuracy: 0.7464\n","Epoch 3/50\n","121/122 [============================>.] - ETA: 0s - loss: 0.4261 - accuracy: 0.8352"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 367). These functions will not be directly callable after loading.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r122/122 [==============================] - 37s 304ms/step - loss: 0.4259 - accuracy: 0.8354 - val_loss: 0.5714 - val_accuracy: 0.7485\n","Epoch 4/50\n","121/122 [============================>.] - ETA: 0s - loss: 0.3598 - accuracy: 0.8740"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 367). These functions will not be directly callable after loading.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r122/122 [==============================] - 39s 321ms/step - loss: 0.3598 - accuracy: 0.8738 - val_loss: 0.5465 - val_accuracy: 0.7546\n","Epoch 5/50\n","122/122 [==============================] - 28s 229ms/step - loss: 0.3014 - accuracy: 0.9040 - val_loss: 0.5639 - val_accuracy: 0.7402\n","Epoch 6/50\n","121/122 [============================>.] - ETA: 0s - loss: 0.2556 - accuracy: 0.9282"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 367). These functions will not be directly callable after loading.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r122/122 [==============================] - 60s 491ms/step - loss: 0.2554 - accuracy: 0.9283 - val_loss: 0.5216 - val_accuracy: 0.7588\n","Epoch 7/50\n","122/122 [==============================] - 25s 202ms/step - loss: 0.2192 - accuracy: 0.9461 - val_loss: 0.5294 - val_accuracy: 0.7546\n","Epoch 8/50\n","122/122 [==============================] - 22s 182ms/step - loss: 0.1811 - accuracy: 0.9621 - val_loss: 0.5271 - val_accuracy: 0.7588\n","Epoch 9/50\n","122/122 [==============================] - 24s 197ms/step - loss: 0.1549 - accuracy: 0.9737 - val_loss: 0.5386 - val_accuracy: 0.7485\n","Epoch 10/50\n","122/122 [==============================] - 23s 187ms/step - loss: 0.1320 - accuracy: 0.9843 - val_loss: 0.5307 - val_accuracy: 0.7670\n","Epoch 11/50\n","122/122 [==============================] - 22s 176ms/step - loss: 0.1111 - accuracy: 0.9892 - val_loss: 0.5260 - val_accuracy: 0.7567\n","Training accuracy was:  0.8932826681570574\n","Validation accuracy was:  0.7480787309733304\n"]}]},{"cell_type":"code","source":["#Roll back to model found performing best on validation set during training.\n","final_model.load_weights(\"./final_BERT_conv_model\")\n","\n","#Generate predictions using this model\n","predictions = final_model.predict(X_test)\n","\n","y_pred = []\n","for pred in predictions:\n","  label = np.argmax(pred)\n","  y_pred.append(label)\n","\n","y_real = []\n","for encoding in y_test:\n","  label = np.argmax(encoding)\n","  y_real.append(label)\n","\n","#Evaluate performance of the model using the current set of hyperparameters\n","evaluation = mce.evaluate_performance(y_pred, y_real)\n","\n","print(\"Tested final model on the test set\")\n","for metric in evaluation:\n","  print(metric, \": \", evaluation[metric], \"\\n\")\n","print(\"\\n \\n \\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"an62p10oLckX","executionInfo":{"status":"ok","timestamp":1686571216582,"user_tz":-120,"elapsed":4560,"user":{"displayName":"Robin Oostveen","userId":"05439229719617169461"}},"outputId":"a9d691d5-d8df-438e-b85e-2da17d9b5b51"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["16/16 [==============================] - 3s 162ms/step\n","Tested final model on the test set\n","Accuracy :  77.9381 \n","\n","Base Positive :  {'TP': 96, 'FP': 49, 'TN': 306, 'FN': 34} \n","\n","Base Neutral :  {'TP': 239, 'FP': 44, 'TN': 156, 'FN': 46} \n","\n","Base Negative :  {'TP': 43, 'FP': 14, 'TN': 401, 'FN': 27} \n","\n","Advanced Positive :  {'Precision': 0.6620689655172414, 'Recall': 0.7384615384615385, 'Specificity': 0.8619718309859155} \n","\n","Advanced Neutral :  {'Precision': 0.8445229681978799, 'Recall': 0.8385964912280702, 'Specificity': 0.78} \n","\n","Advanced Negative :  {'Precision': 0.7543859649122807, 'Recall': 0.6142857142857143, 'Specificity': 0.9662650602409638} \n","\n","Balanced Accuracy :  0.7304479146584409 \n","\n","F_Score :  0.40840716375139613 \n","\n","\n"," \n"," \n","\n"]}]},{"cell_type":"code","source":["print(mce.confusion_matrix(y_pred, y_real))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b-SIms3QrklF","executionInfo":{"status":"ok","timestamp":1686571250987,"user_tz":-120,"elapsed":601,"user":{"displayName":"Robin Oostveen","userId":"05439229719617169461"}},"outputId":"18cfd86e-b699-4282-fd3e-bc9478ac0e28"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 43   7   7]\n"," [ 17 239  27]\n"," [ 10  39  96]]\n"]}]},{"cell_type":"code","source":["#Save the trained model\n","final_model.save(\"/content/drive/My Drive/Bachelor Scriptie KI/Programming/Notebooks/financial_BERT_CNN_model\")"],"metadata":{"id":"xb48jiyFoZAI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686571277665,"user_tz":-120,"elapsed":15047,"user":{"displayName":"Robin Oostveen","userId":"05439229719617169461"}},"outputId":"8b442d09-96ac-4e5c-b343-d88124d908cd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 367). These functions will not be directly callable after loading.\n"]}]},{"cell_type":"markdown","source":["#Sentence-level error inspection"],"metadata":{"id":"cpT5Cam4Hx8i"}},{"cell_type":"code","source":["#Load the trained model\n","load_model = tf.keras.models.load_model(\"/content/drive/My Drive/Bachelor Scriptie KI/Programming/Notebooks/financial_BERT_CNN_model\")"],"metadata":{"id":"_cL2Hjy2t4Uu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["load_model.summary()"],"metadata":{"id":"IkCkAfyxe7O5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686737716853,"user_tz":-120,"elapsed":246,"user":{"displayName":"Robin Oostveen","userId":"05439229719617169461"}},"outputId":"418fd3fd-b57b-4af6-9695-ec186413d4b7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_1\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," text_input (InputLayer)        [(None,)]            0           []                               \n","                                                                                                  \n"," keras_layer (KerasLayer)       {'input_mask': (Non  0           ['text_input[0][0]']             \n","                                e, 128),                                                          \n","                                 'input_type_ids':                                                \n","                                (None, 128),                                                      \n","                                 'input_word_ids':                                                \n","                                (None, 128)}                                                      \n","                                                                                                  \n"," keras_layer_1 (KerasLayer)     {'default': (None,   109482241   ['keras_layer[0][0]',            \n","                                768),                             'keras_layer[0][1]',            \n","                                 'pooled_output': (               'keras_layer[0][2]']            \n","                                None, 768),                                                       \n","                                 'encoder_outputs':                                               \n","                                 [(None, 128, 768),                                               \n","                                 (None, 128, 768),                                                \n","                                 (None, 128, 768),                                                \n","                                 (None, 128, 768),                                                \n","                                 (None, 128, 768),                                                \n","                                 (None, 128, 768),                                                \n","                                 (None, 128, 768),                                                \n","                                 (None, 128, 768),                                                \n","                                 (None, 128, 768),                                                \n","                                 (None, 128, 768),                                                \n","                                 (None, 128, 768),                                                \n","                                 (None, 128, 768)],                                               \n","                                 'sequence_output':                                               \n","                                 (None, 128, 768)}                                                \n","                                                                                                  \n"," conv1d (Conv1D)                (None, 128, 256)     196864      ['keras_layer_1[0][14]']         \n","                                                                                                  \n"," pool (GlobalMaxPooling1D)      (None, 256)          0           ['conv1d[0][0]']                 \n","                                                                                                  \n"," output (Dense)                 (None, 3)            771         ['pool[0][0]']                   \n","                                                                                                  \n","==================================================================================================\n","Total params: 109,679,876\n","Trainable params: 197,635\n","Non-trainable params: 109,482,241\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","source":["#Generate predictions using this model\n","predictions_loaded = load_model.predict(X_test)\n","\n","y_pred_loaded = []\n","for pred in predictions_loaded:\n","  label = np.argmax(pred)\n","  y_pred_loaded.append(label)\n","\n","y_real_loaded = []\n","for encoding in y_test:\n","  label = np.argmax(encoding)\n","  y_real_loaded.append(label)\n","\n","#Evaluate performance of the model using the current set of hyperparameters\n","evaluation_loaded = mce.evaluate_performance(y_pred_loaded, y_real_loaded)\n","\n","print(\"Tested final model on the test set\")\n","for metric in evaluation_loaded:\n","  print(metric, \": \", evaluation_loaded[metric], \"\\n\")\n","print(\"\\n \\n \\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"STGB5622e-YI","executionInfo":{"status":"ok","timestamp":1686737732726,"user_tz":-120,"elapsed":11435,"user":{"displayName":"Robin Oostveen","userId":"05439229719617169461"}},"outputId":"a5625656-2165-4bdd-a1ab-97ff5f2f07cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["16/16 [==============================] - 11s 183ms/step\n","Tested final model on the test set\n","Accuracy :  77.9381 \n","\n","Base Positive :  {'TP': 96, 'FP': 49, 'TN': 306, 'FN': 34} \n","\n","Base Neutral :  {'TP': 239, 'FP': 44, 'TN': 156, 'FN': 46} \n","\n","Base Negative :  {'TP': 43, 'FP': 14, 'TN': 401, 'FN': 27} \n","\n","Advanced Positive :  {'Precision': 0.6620689655172414, 'Recall': 0.7384615384615385, 'Specificity': 0.8619718309859155} \n","\n","Advanced Neutral :  {'Precision': 0.8445229681978799, 'Recall': 0.8385964912280702, 'Specificity': 0.78} \n","\n","Advanced Negative :  {'Precision': 0.7543859649122807, 'Recall': 0.6142857142857143, 'Specificity': 0.9662650602409638} \n","\n","Balanced Accuracy :  0.7304479146584409 \n","\n","F_Score :  0.40840716375139613 \n","\n","\n"," \n"," \n","\n"]}]},{"cell_type":"code","source":["print(mce.confusion_matrix(y_pred_loaded, y_real_loaded))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uRXykme338hG","executionInfo":{"status":"ok","timestamp":1686737732726,"user_tz":-120,"elapsed":29,"user":{"displayName":"Robin Oostveen","userId":"05439229719617169461"}},"outputId":"ba0854bf-89e1-4bb5-e9ad-cd7f17facb8f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 43   7   7]\n"," [ 17 239  27]\n"," [ 10  39  96]]\n"]}]},{"cell_type":"code","source":["def get_specific_errors(dataframe, y_pred, y_real, vertical, horizontal):\n","\t\"\"\"\n","\tGet the indexes from specific cells in the confusion matrix.\n","\n","\tparam dataframe: Pandas DataFrame containing the sentences and indices.\n","\tparam y_pred: (list) contains the predicted sentiments.\n","\tparam y_real: (list) contains the real sentiments.\n","\tparam vertical: (int) corresponds to the column in the confusion matrix.\n","\tparam horizontal: (int) corresponds to the row in the confusion matrix.\n","\t\"\"\"\n","\ti = 0\n","\terrors = []\n","\twhile i < len(dataframe.Sentence):\n","\t\tif (horizontal == y_pred[i]) and (vertical == y_real[i]):\n","\t\t\terrors.append(dataframe.index[i])\n","\t\ti += 1\n","\treturn errors"],"metadata":{"id":"tb_8mKzmZCjR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Neutral sentences predicted as positive.\n","pos_neu_errors_index = get_specific_errors(test, y_pred_loaded, y_real_loaded, 1, 2)"],"metadata":{"id":"svtQlv79JQOg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(\"/content/drive/MyDrive/Bachelor Scriptie KI/Programming/Notebooks/Error Indexes/Financial/updated_DL_financial_pos_neu.txt\", \"w\") as writefile:\n","  for index in pos_neu_errors_index:\n","    writefile.write(str(index))\n","    writefile.write(\"\\n\")"],"metadata":{"id":"9gfq3oMBcPQ3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Positive sentences predicted as neutral.\n","neu_pos_errors_index = get_specific_errors(test, y_pred_loaded, y_real_loaded, 2, 1)"],"metadata":{"id":"-tCwT-fccTTn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(\"/content/drive/MyDrive/Bachelor Scriptie KI/Programming/Notebooks/Error Indexes/Financial/updated_DL_financial_neu_pos.txt\", \"w\") as writefile:\n","  for index in neu_pos_errors_index:\n","    writefile.write(str(index))\n","    writefile.write(\"\\n\")"],"metadata":{"id":"FtPnwdHNdZb7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#True positives for the neutral class.\n","tp_neu_index = get_specific_errors(test, y_pred_loaded, y_real_loaded, 1, 1)"],"metadata":{"id":"X_9Fo4VV2_kl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(\"/content/drive/MyDrive/Bachelor Scriptie KI/Programming/Notebooks/Error Indexes/Financial/updated_DL_financial_tp_neu.txt\", \"w\") as writefile:\n","  for index in tp_neu_index:\n","    writefile.write(str(index))\n","    writefile.write(\"\\n\")"],"metadata":{"id":"f-1KXxav3Fps"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Negative sentences predicted as neutral.\n","neu_neg_errors_index = get_specific_errors(test, y_pred_loaded, y_real_loaded, 0, 1)"],"metadata":{"id":"pw1h_5WIdf0w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(\"/content/drive/MyDrive/Bachelor Scriptie KI/Programming/Notebooks/Error Indexes/Financial/updated_DL_financial_neu_neg.txt\", \"w\") as writefile:\n","  for index in neu_neg_errors_index:\n","    writefile.write(str(index))\n","    writefile.write(\"\\n\")"],"metadata":{"id":"DT5zKFXxd-SR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Positive sentences predicted as negative.\n","pos_neg_errors_index = get_specific_errors(test, y_pred_loaded, y_real_loaded, 0, 2)"],"metadata":{"id":"dDqHUEWveCaP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(\"/content/drive/MyDrive/Bachelor Scriptie KI/Programming/Notebooks/Error Indexes/Financial/updated_DL_financial_pos_neg.txt\", \"w\") as writefile:\n","  for index in pos_neg_errors_index:\n","    writefile.write(str(index))\n","    writefile.write(\"\\n\")"],"metadata":{"id":"z36LvxX0eHyw"},"execution_count":null,"outputs":[]}]}